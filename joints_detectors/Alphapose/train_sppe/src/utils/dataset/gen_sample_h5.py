import h5py
import json
import logging
import os
import numpy as np
import argparse
from tqdm import tqdm
import copy
import glob
import cv2

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def cvt_output2coco(data_dir, image_dir, output):
    '''
    Convert images and npz to coco format
    The outputs are generated by demo.py

    data_dir/
        image_dir/
            1.jpg
            1_hms_data.npz
            2.jpg
            2_hms_data.npz

    file_name = image_dir/1.jpg
    '''
    logger.info('convert outputs to coco format')
    images_info = []
    anno_info = []
    pose_id = 0
    for file_name in glob.glob(os.path.join(data_dir, image_dir, '[0-9]*[0-9].jpg')):
        image_id = int(file_name.split('/')[-1].split('.')[0])
        image_name = file_name[file_name.index(image_dir):]
        images_info.append({'file_name':image_name, 'id':image_id})
        data_npz = file_name.replace('.jpg', '_hms_data.npz')
        if not os.path.exists(data_npz):
            continue
        pose_list = np.load(data_npz, allow_pickle=True)['result'].item()['result']
        for pose in pose_list:
            kps = pose['keypoints']
            sco = pose['kp_score']
            coco_kps = []
            for i in range(kps.shape[0]):
                coco_kps.append(kps[i][0].item())
                coco_kps.append(kps[i][1].item())
                coco_kps.append(1 if sco[i].item() > 0.05 else 0)
            anno_info.append({'keypoints':coco_kps, 'image_id':image_id, 'id':pose_id, 'category_id':1, 'score':sco[i].item()})
            pose_id += 1
    
    with open(output, 'w') as f:
        output_dict = {'images':images_info, 'annotations':anno_info} 
        f.write(json.dumps(output_dict))
    with open(output.replace('.json', '_anno.json'), 'w') as f:
        f.write(json.dumps(anno_info))

def cvt_bbox2evalformat(bbox_path, out_image_path, out_h5):
    '''
    Convert output npz file to an image list txt and a bndbox h5 for evaluation (train_sppe/src/evaluation.py)
    '''
    bboxes = np.load(bbox_path, allow_pickle=True)
    b_images = bboxes['images']
    b_boxes = bboxes['boxes']
    assert len(b_images) == len(b_boxes), 'boxes length: {} images length {}'.format(len(b_boxes), len(b_images))

    imglist_file = open(out_image_path, 'w')
    bb_h5 = h5py.File(out_h5, 'w')

    bb = {'xmin':[], 'ymin':[], 'xmax':[], 'ymax':[]}
    for i in range(len(b_images)):
        if b_boxes[i] is not None:
            for box in b_boxes[i]:
                bb['xmin'].append(box[0])
                bb['ymin'].append(box[1])
                bb['xmax'].append(box[2])
                bb['ymax'].append(box[3])
                imglist_file.write(b_images[i] + '\n')
    bb_h5['xmin'] = bb['xmin']
    bb_h5['ymin'] = bb['ymin']
    bb_h5['xmax'] = bb['xmax']
    bb_h5['ymax'] = bb['ymax']

    imglist_file.close()
    bb_h5.close()

def gen_h36m_h5(train_sample_path, val_sample_path, output_path):
    '''
    Convert h36m annotation file to input h5 format for detection
    '''
    def append_samples(path, imgname_list, part_list, bndbox_list):
        logger.info('Process {}'.format(path))
        with open(path, 'r') as f:
            ann = json.load(f)
            num = len(ann['images'])
            for i in range(num):
                imgname = ann['images'][i]['file_name']
                part = np.array(ann['annotations'][i]['keypoints_img'])
                part_vis = ann['annotations'][i]['keypoints_vis']
                for j in range(len(part)):
                    if not part_vis[j]:
                        part[j] *= -1
                bndbox = np.array(ann['annotations'][i]['bbox'])                
                imgname_list.append(imgname.encode())
                part_list.append(part)
                bndbox_list.append(bndbox)
    
    imgname_list, part_list, bndbox_list = [], [], []
    for path in train_sample_path:
        append_samples(path, imgname_list, part_list, bndbox_list)
    logger.info('Number of train samples {}'.format(len(imgname_list)))
    for path in val_sample_path:
        append_samples(path, imgname_list, part_list, bndbox_list)
    logger.info('Number of all samples {}'.format(len(imgname_list)))
    assert len(imgname_list) == len(part_list) and len(imgname_list) == len(bndbox_list)

    for i in range(len(part_list)-1):
        assert part_list[i].shape == part_list[i+1].shape, 'shape_{:d} {} shape_{:d} {}'.format(i, part_list[i].shape, i+1, part_list[i+1].shape)

    f = h5py.File(output_path, 'w')
    f['imgname'] = imgname_list
    f['part'] = part_list
    f['bndbox'] = bndbox_list
    f.close()

def merge_dtbox_gtjoints(box_npz, joint_h5, output_path):
    '''
    Merge bounding box output file with joints' groundtruth for training sppe.
    '''
    boxes = np.load(box_npz, allow_pickle=True)
    jinfo = h5py.File(joint_h5, 'r')
    b_images = boxes['images']
    b_boxes = boxes['boxes']
    j_images = np.array(jinfo['imgname'])
    j_joints = jinfo['part']
    j_bndbox = np.array(jinfo['bndbox'])

    assert len(b_images) == len(j_images), 'detected images count {:d} groundtruth images count {:d}'.format(len(b_images), len(j_images))
    images_num = len(b_images)

    out_images, out_part, out_bndbox = [], [], []
    images_num_desc = tqdm(range(images_num))
    for i in images_num_desc:
        if b_boxes[i] is not None:
            imgname = b_images[i].split('/')[-2:]
            imgname = os.path.join(imgname[0], '{}_{:06d}.jpg'.format(imgname[0],int(imgname[1].split('.')[0]))).encode()
            j = int(np.where(j_images == imgname)[0][0])
            for b in range(len(b_boxes[i])):
                i_lt = np.minimum(b_boxes[i][b,:2], j_bndbox[j][:2])
                i_rb = np.maximum(b_boxes[i][b,2:], j_bndbox[j][2:])
                it_area = (i_rb[0] - i_lt[0]) * (i_rb[1] - i_lt[1])
                if it_area > 0:
                    a_wh = b_boxes[i][b,2:] - b_boxes[i][b,:2]
                    b_wh = j_bndbox[j][2:] - j_bndbox[j][:2]
                    if it_area / (a_wh[0] * a_wh[1] + b_wh[0] * b_wh[1]) > 0.6:
                        out_images.append(j_images[j])
                        out_bndbox.append(b_boxes[i][b])
                        out_part.append(j_joints[j])
    
    for i in images_num_desc:
        imgname = out_images[i]
    
    f = h5py.File(output_path, 'w')
    f['imgname'] = out_images
    f['part'] = out_part
    f['bndbox'] = out_bndbox
    f.close()

def rearrange_h5(h5_path, output_path):
    # place training data before validation data
    h36m_training = ['01', '05', '06', '07', '08']
    h36m_validation = ['09', '11']

    f = h5py.File(h5_path, 'r')
    img_list = f['imgname'][:]
    part_list = f['part'][:]
    bndbox_list = f['bndbox'][:]
    f.close()

    hm = {}
    image_num = len(img_list)

    for i in range(image_num):
        subj = img_list[i].decode().split('_')[1]
        if subj not in hm:
            hm[subj] = {'imgname':[], 'part':[], 'bndbox':[]}
        hm[subj]['imgname'].append(img_list[i])
        hm[subj]['part'].append(part_list[i])
        hm[subj]['bndbox'].append(bndbox_list[i])

    w_img_list = []
    w_part_list = []
    w_bndbox_list = []
    for subj in h36m_training:
        w_img_list += hm[subj]['imgname']
        w_part_list += hm[subj]['part']
        w_bndbox_list += hm[subj]['bndbox']
    
    print('start of validation:', len(w_img_list))

    for subj in h36m_validation:
        w_img_list += hm[subj]['imgname']
        w_part_list += hm[subj]['part']
        w_bndbox_list += hm[subj]['bndbox']

    f = h5py.File(output_path, 'w')
    f['imgname'] = w_img_list
    f['part'] = w_part_list
    f['bndbox'] = w_bndbox_list
    f.close()

def check_h5(h5_path):
    f = h5py.File(h5_path, 'r')
    img_list = f['imgname'][:]
    part_list = f['part'][:]
    bndbox_list = f['bndbox'][:]
    f.close()

    hm = {}
    image_num = len(img_list)

    for i in range(image_num):
        subj = img_list[i].decode().split('_')[1]
        if subj not in hm:
            hm[subj] = {'imgname':[], 'part':[], 'bndbox':[]}
        hm[subj]['imgname'].append(img_list[i])
        hm[subj]['part'].append(part_list[i])
        hm[subj]['bndbox'].append(bndbox_list[i])
    
    def draw_pose(path, kps, box):
        
        img = cv2.imread(os.path.join('/Volumes/Dog/Data/pose/h36m_images_unzip/images', path.decode()))
        for i in range(kps.shape[0]):
            x = int(kps[i][0])
            y = int(kps[i][1])
            cv2.circle(img, (x, y), 3, (0,0,255), -1)
        
        cv2.rectangle(img, (int(box[0]), int(box[1])),(int(box[2]), int(box[3])),(55,255,155), 3)
        return img
        # w = box[2] - box[0] + 1
        # h = box[3] - box[1] + 1
        # x0, y0 = int(box[0]), int(box[1])
        # x1, y1 = x0, int(box[1]+h)
        # x2, y2 = int(box[0]+w), y1
        # x3, y3 = x2, y0
        # cv2.line(img, (x0, y0), (x1, y1), )

    for k,h in hm.items():
        n = len(h['imgname'])
        print(k, n)
        for i in np.random.randint(n, size=20):
            name = h['imgname'][i]
            part = h['part'][i]
            box  = h['bndbox'][i]
            img = draw_pose(name, part, box)
            cv2.imshow('pose', img)
            cv2.waitKey()

def merge_dtboxes(box_path, end_index, output_path):
    images, boxes = [], []
    for i in range(end_index + 1):
        path = '{}_{}.npz'.format(box_path, i)
        if os.path.exists(path):
            logger.info('merge {}'.format(path))
            f = np.load(path, allow_pickle=True)
            images.append(f['images'])
            boxes.append(f['boxes'])
    np.savez(output_path, images=np.concatenate(images, axis=0), boxes=np.concatenate(boxes, axis=0))

h36m_config = {
    'train_list':['Human36M_subject1.json', 'Human36M_subject5.json','Human36M_subject6.json', 'Human36M_subject7.json', 'Human36M_subject8.json'],
    'val_list':['Human36M_subject9.json', 'Human36M_subject11.json'],    
}

def parse_args():
    parser = argparse.ArgumentParser(description='Preparation of data')
    parser.add_argument('-o', '--output', type=str, help='output file path')
    parser.add_argument('-i', '--input', type=str, help='input file path')
    parser.add_argument('--detect-box-path', type=str, help='bounding box npz file outputed by gen_train_bbox.py')
    parser.add_argument('--gt-joint-path', type=str, help='joints groundtruth file path')
    parser.add_argument('-m', '--mode', type=str, help='mode, h36m2h5|merge_box_gt|box2eval|merge_boxes|rearrange_h5|cvt_coco|check_pose')
    parser.add_argument('-d', '--dir', type=str, help='data directory')
    return parser.parse_args()

if __name__ == '__main__':
    args = parse_args()
    logger.info(args)
    '''
    ==============================================
    Step of preparing data for training h36m data
    1. Generate bounding box detected by yolo using gen_train_bbox.py
    2. If generate multi bbox files, merge files by calling merge_dtboxes
    3. Generate h36m keypoints groundtruth by calling gen_h36m_h5
    4. Merge bbox and h36m groundtruth by calling merge_dtbox_gtjoints
    5. Rearrange merged file to garantee training data ahead

    ==============================================
    Step of preparing data for validation
    1. Generate bounding box detected by yolo using gen_train_bbox.py
    2. Call cvt_bbox2evalformat

    ==============================================
    Step of converting data to coco format
    '''
    if args.mode == 'h36m2h5':
        train_list = [os.path.join(args.dir, p) for p in h36m_config['train_list']]
        val_list = [os.path.join(args.dir, p) for p in h36m_config['val_list']]
        gen_h36m_h5(train_list, val_list, args.output)
    elif args.mode == 'merge_box_gt':
        merge_dtbox_gtjoints(args.detect_box_path, args.gt_joint_path, args.output)
    elif args.mode == 'box2eval':
        imglist_path, bbh5_path = args.output.split(',')
        cvt_bbox2evalformat(args.detect_box_path, imglist_path, bbh5_path)
    elif args.mode == 'merge_boxes':
        arr = args.detect_box_path.split(',')
        path = arr[0]
        end_index = int(arr[1])
        merge_dtboxes(path, end_index, args.output)
    elif args.mode == 'rearrange_h5':
        rearrange_h5(args.input, args.output)
    elif args.mode == 'cvt_coco':
        cvt_output2coco(args.dir, args.input, args.output)
    elif args.mode == 'check_pose':
        check_h5(args.input)


    
